{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd71ac1d",
   "metadata": {},
   "source": [
    "# NN Project, topic #4: Language identification from a text corpus\n",
    "\n",
    "Students: Shahzaib MUHAMMAD, Adriana NICOARA, Scott TANKARD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631d741",
   "metadata": {},
   "source": [
    "## Our group's topic\n",
    "\n",
    "From: https://arche.univ-lorraine.fr/mod/page/view.php?id=1310565\n",
    "\n",
    "Project 4: Language identification from a text corpus\n",
    "\n",
    "Students: Scott TANKARD, Adriana NICOARA, Shahzaib MUHAMMAD\n",
    "\n",
    "Task: Text classification\n",
    "\n",
    "Model: Feedforward neural network\n",
    "\n",
    "Dataset: https://www.kaggle.com/zarajamshaid/language-identification-datasst\n",
    "\n",
    "Hint: As a preprocessing step, you should transforms the sentences into trigrams at the character level (more details at: https://towardsdatascience.com/deep-neural-network-language-identification-ae1c158f6a7d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d8f63",
   "metadata": {},
   "source": [
    "## General project instructions\n",
    "\n",
    "From: https://arche.univ-lorraine.fr/mod/page/view.php?id=1301340\n",
    "\n",
    "General project instructions\n",
    "\n",
    "Project structure\n",
    "\n",
    "Each project follows the same overall structure:\n",
    "\n",
    "    Download the data and preprocess it as required for the given task (if needed).\n",
    "    Write a Dataset class for creating the train and test datasets (and corresponding dataloaders).\n",
    "    Define the neural network model.\n",
    "    Define the hyperparameters to create an instance of the model (e.g., hidden space size, number of convolution kernels...) as well as the parameters required to train neural network (e.g., learning rate).\n",
    "    Write the training loop for training the model.\n",
    "    Evaluate the model on the test data. In this part, it is expected to choose an appropriate evaluation metric based on your task. For instance, for classification task, accuracy should be computed (but you can also search for 'precision' and 'recall').\n",
    "    Save the trained model parameters, and the obtained results if needed.\n",
    "\n",
    "\n",
    "Deliverables\n",
    "\n",
    "The project should be written using Pytorch (and not Keras/Tensorflow or any other python deep learning framework). You're expected to send a zip file containing:\n",
    "\n",
    "    A python file implementing steps 1 to 7. This can be either a jupyter notebook or a .py script. A single file is preferred, but you can use several files if that's your style, as long as your main executable script is clearly indicated (for instance it's named 'main.py').\n",
    "    The obtained results as extra files (e.g., the trained model parameters, a figure with the training loss / validation metric over epochs, a graph comparing different architectures if you want to play arround with it, etc.)\n",
    "\n",
    "Your code should be commented in order to clarify implementation details, and desciption about the inputs and outputs of functions.\n",
    "\n",
    "Note: There is no need to send the dataset, since I will download it and run your scripts directly. Therefore, make sure that you do not transform/change the raw data, and if there is some preprocessing involved, include it in your python file(s).\n",
    "\n",
    "\n",
    "General Hints\n",
    "\n",
    "    The tasks corresponding to these projects have been extensively studied. Don't hesitate to search online for more information (either tutorials or research papers, even pieces of code if you can adapt it).\n",
    "    Some datasets are very large. Therefore, you don't need to use all the data, but instead you can extract a subset of it (for instance, only a few languages for language identification, only a few images/classes for image recognition etc.) to have a lighter dataset / model / training procedure.\n",
    "    Some datasets are provided with a train / test split, but not always. Either way, you can create your own split (a good rule of thumb can be 80% training and 20 % testing).\n",
    "    It is strongly advised to use validation in order to monitor training (see 'bonus work' in lab 2.2). You can use part of the training data (e.g., 10%) as a validation set.\n",
    "    It is good practice to start with a light model (very few layers/parameters) and dataset (subset of your whole dataset) for prototyping. The performance won't be very good, but it's useful to check if there are any error in the train/test procedure. Once everything runs smoothly, you can increase the size of the model and use more data.\n",
    "    If your project includes convolutional or recurrent neural networks, don't wait the corresponding lab: you can already start working on it (you can basically do everything, just using a 'dummy' MLP model instead of a CNN/RNN).\n",
    "\n",
    "Modifié le: vendredi 21 octobre 2022, 12:46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7797bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS:\n",
    "filterLanguage=False #Either Filter the dataset or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3fb448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Download the data and preprocess it as required for the given task (if needed).\n",
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c31d806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22000 entries, 0 to 21999\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Text      22000 non-null  object\n",
      " 1   language  22000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 343.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./dataset.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e57db9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>エノが行きがかりでバスに乗ってしまい、気分が悪くなった際に助けるが、今すぐバスを降りたいと運...</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tsutinalar i̇ngilizce tsuutina kanadada albert...</td>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>müller mox figura centralis circulorum doctoru...</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>برقی بار electric charge تمام زیرجوہری ذرات کی...</td>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>シャーリー・フィールドは、サン・ベルナルド・アベニュー沿い市民センターとrtマーティン高校に...</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  language\n",
       "0  klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
       "1  sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2  ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3  விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4  de spons behoort tot het geslacht haliclona en...     Dutch\n",
       "5  エノが行きがかりでバスに乗ってしまい、気分が悪くなった際に助けるが、今すぐバスを降りたいと運...  Japanese\n",
       "6  tsutinalar i̇ngilizce tsuutina kanadada albert...   Turkish\n",
       "7  müller mox figura centralis circulorum doctoru...     Latin\n",
       "8  برقی بار electric charge تمام زیرجوہری ذرات کی...      Urdu\n",
       "9  シャーリー・フィールドは、サン・ベルナルド・アベニュー沿い市民センターとrtマーティン高校に...  Japanese"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a60848ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Estonian' 'Swedish' 'Thai' 'Tamil' 'Dutch' 'Japanese' 'Turkish' 'Latin'\n",
      " 'Urdu' 'Indonesian' 'Portugese' 'French' 'Chinese' 'Korean' 'Hindi'\n",
      " 'Spanish' 'Pushto' 'Persian' 'Romanian' 'Russian' 'English' 'Arabic']\n",
      "Number of unique languages:  22\n"
     ]
    }
   ],
   "source": [
    "unique_lang = data[\"language\"].unique()\n",
    "print(unique_lang)\n",
    "print(\"Number of unique languages: \", len(unique_lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22b2cc",
   "metadata": {},
   "source": [
    "We choose 7 languages to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d1ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trim = pd.DataFrame(columns=['Text','language'])\n",
    "lang = None\n",
    "if(filterLanguage):\n",
    "    lang = ['Estonian', 'Swedish', 'Indonesian','Turkish','French', 'Romanian', 'English' ]\n",
    "    data_trim = pd.DataFrame(columns=['Text','language'])\n",
    "    for l in lang:\n",
    "        lang_trim = data[data['language'] ==l].sample(1000, random_state = 100)\n",
    "        data_trim = data_trim.append(lang_trim)\n",
    "else:\n",
    "    lang = data['language'].unique()\n",
    "    data_trim = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc003fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1B: Write a Dataset class for creating the train and test datasets.\n",
    "#########\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data_trim[\"Text\"]\n",
    "y = data_trim[\"language\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42,stratify=y)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "19b3b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_trigrams(corpus,n_feat=200):\n",
    "    \"\"\"\n",
    "    Returns a list of the N most common character trigrams from a list of sentences\n",
    "    params\n",
    "    ------------\n",
    "        corpus: list of strings\n",
    "        n_feat: integer\n",
    "    \"\"\"\n",
    "    #fit the n-gram model\n",
    "    vectorizer = CountVectorizer(analyzer='char_wb',\n",
    "                            ngram_range=(3, 3),\n",
    "                            max_features=n_feat)\n",
    "    \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    #Get model feature names\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "37241ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "features_set = set()\n",
    "\n",
    "for l in lang:\n",
    "    #get corpus filtered by language\n",
    "    corpus = train_data[train_data.language==l]['Text']\n",
    "    #get 200 most frequent trigrams\n",
    "    trigrams = get_trigrams(corpus)\n",
    "    \n",
    "    #add to dict and set\n",
    "    features[l] = trigrams \n",
    "    features_set.update(trigrams)\n",
    "#create vocabulary list using feature set\n",
    "vocab = dict()\n",
    "for i,f in enumerate(features_set):\n",
    "    vocab[f]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train count vectoriser using vocabulary\n",
    "vectorizer = CountVectorizer(analyzer='char_wb',\n",
    "                             ngram_range=(3, 3),\n",
    "                             vocabulary=vocab)\n",
    "\n",
    "#create feature matrix for training set\n",
    "corpus = train_data['Text']   \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "train_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "533c7831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>परि</th>\n",
       "      <th>se</th>\n",
       "      <th>されて</th>\n",
       "      <th>out</th>\n",
       "      <th>تھے</th>\n",
       "      <th>की</th>\n",
       "      <th>يد</th>\n",
       "      <th>میٹ</th>\n",
       "      <th>li</th>\n",
       "      <th>نند</th>\n",
       "      <th>...</th>\n",
       "      <th>мен</th>\n",
       "      <th>și</th>\n",
       "      <th>ना</th>\n",
       "      <th>وند</th>\n",
       "      <th>्र</th>\n",
       "      <th>राज</th>\n",
       "      <th>त्र</th>\n",
       "      <th>sia</th>\n",
       "      <th>하는</th>\n",
       "      <th>กวา</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18695</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18696</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18697</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18698</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18699</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18700 rows × 2936 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       परि  se   されて  out  تھے   की  يد   میٹ   li  نند  ...  мен  și    ना  \\\n",
       "0        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "1        0    1    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2        0    1    0    0    0    0    0    0    0    0  ...    0    3    0   \n",
       "3        0    0    0    0    0    0    0    0    0    0  ...    3    0    0   \n",
       "4        0    0    0    0    0    1    0    0    0    0  ...    0    0    1   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "18695    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "18696    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "18697    0    0    0    0    0    0    0    0    0    0  ...    0    2    0   \n",
       "18698    0    2    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "18699    0    0    0    0    0    0    0    0    0    0  ...    0    2    0   \n",
       "\n",
       "       وند  ्र   राज  त्र  sia   하는  กวา  \n",
       "0        0    0    0    0    0    0    0  \n",
       "1        0    0    0    0    0    0    0  \n",
       "2        0    0    0    0    0    0    0  \n",
       "3        0    0    0    0    0    0    0  \n",
       "4        0    1    3    0    0    0    0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  \n",
       "18695    0    0    0    0    0    0    0  \n",
       "18696    0    0    0    0    0    0    0  \n",
       "18697    0    0    0    0    0    0    0  \n",
       "18698    0    0    0    0    0    0    0  \n",
       "18699    0    0    0    0    0    0    0  \n",
       "\n",
       "[18700 rows x 2936 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0c6388f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale feature matrix \n",
    "train_min = train_feat.min()\n",
    "train_max = train_feat.max()\n",
    "train_feat = (train_feat - train_min)/(train_max-train_min)\n",
    "\n",
    "#Add target variable \n",
    "train_feat['lang'] = list(train_data['language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549261c",
   "metadata": {},
   "source": [
    "Matrix data that will be used for training: train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4683ccf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>परि</th>\n",
       "      <th>se</th>\n",
       "      <th>されて</th>\n",
       "      <th>out</th>\n",
       "      <th>تھے</th>\n",
       "      <th>की</th>\n",
       "      <th>يد</th>\n",
       "      <th>میٹ</th>\n",
       "      <th>li</th>\n",
       "      <th>نند</th>\n",
       "      <th>...</th>\n",
       "      <th>și</th>\n",
       "      <th>ना</th>\n",
       "      <th>وند</th>\n",
       "      <th>्र</th>\n",
       "      <th>राज</th>\n",
       "      <th>त्र</th>\n",
       "      <th>sia</th>\n",
       "      <th>하는</th>\n",
       "      <th>กวา</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Persian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18695</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Pushto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18696</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18697</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18698</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18699</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18700 rows × 2937 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       परि   se   されて  out  تھے        की  يد   میٹ   li  نند  ...  și   \\\n",
       "0      0.0  0.00  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "1      0.0  0.05  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "2      0.0  0.05  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...  0.3   \n",
       "3      0.0  0.00  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "4      0.0  0.00  0.0  0.0  0.0  0.071429  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "...    ...   ...  ...  ...  ...       ...  ...  ...  ...  ...  ...  ...   \n",
       "18695  0.0  0.00  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "18696  0.0  0.00  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "18697  0.0  0.00  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...  0.2   \n",
       "18698  0.0  0.10  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "18699  0.0  0.00  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...  0.2   \n",
       "\n",
       "             ना  وند       ्र   राज  त्र  sia   하는  กวา       lang  \n",
       "0      0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0    Persian  \n",
       "1      0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0     French  \n",
       "2      0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   Romanian  \n",
       "3      0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0    Russian  \n",
       "4      0.066667  0.0  0.142857  0.2  0.0  0.0  0.0  0.0      Hindi  \n",
       "...         ...  ...       ...  ...  ...  ...  ...  ...        ...  \n",
       "18695  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0     Pushto  \n",
       "18696  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0    Russian  \n",
       "18697  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   Romanian  \n",
       "18698  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  Portugese  \n",
       "18699  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   Romanian  \n",
       "\n",
       "[18700 rows x 2937 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ad72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ab01f",
   "metadata": {},
   "source": [
    "Heatmap representing the number of common trigrams between languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Calculate number of shared trigrams\n",
    "labels = ['Estonian', 'Swedish', 'Indonesian','Turkish','French', 'Romanian', 'English' ]\n",
    "mat = []\n",
    "for i in labels:\n",
    "    vec = []\n",
    "    for j in labels:\n",
    "        l1 = features[i]\n",
    "        l2 = features[j]\n",
    "        intersec = [l for l in l1 if l in l2] \n",
    "        # print(intersec)\n",
    "        vec.append(len(intersec))\n",
    "    mat.append(vec)\n",
    "\n",
    "print(mat)\n",
    "\n",
    "#Plot heatmap\n",
    "lang = ['Estonian', 'Swedish', 'Indonesian','Turkish','French', 'Romanian', 'English' ]\n",
    "conf_matrix_df = pd.DataFrame(mat,columns=lang,index=lang)\n",
    "\n",
    "print(conf_matrix_df)\n",
    "mask = [[ True,  False,  False,  False,  False,  False, False],\n",
    "       [True,  True,  False,  False,  False,  False, False],\n",
    "       [True, True,  True,  False,  False,  False, False],\n",
    "       [True, True, True,  True,  False,  False, False],\n",
    "       [True, True, True, True,  True,  False, False],\n",
    "       [True, True, True, True, True, True, False],\n",
    "       [True, True, True, True, True, True, True]]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10), facecolor='w', edgecolor='k')\n",
    "# sns.set(font_scale=1.5)\n",
    "sns.heatmap(conf_matrix_df,cmap='coolwarm',annot=True,fmt='.5g',cbar=False)\n",
    "\n",
    "plt.savefig('../feat_explore.png',format='png',dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198902ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22129d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c71077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Write a Dataset class for creating the train and test datasets (and corresponding dataloaders).\n",
    "#########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Define the neural network model.\n",
    "#########\n",
    "\n",
    "# https://towardsdatascience.com/deep-neural-network-language-identification-ae1c158f6a7d\n",
    "\n",
    "try:\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from keras.utils import np_utils\n",
    "\n",
    "    def fit_encoder():\n",
    "        #Fit encoder\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(['deu', 'eng', 'fra', 'ita', 'por', 'spa'])\n",
    "\n",
    "    def encode(y):\n",
    "        \"\"\"\n",
    "        Returns a list of one hot encodings\n",
    "        Params\n",
    "        ---------\n",
    "            y: list of language labels\n",
    "        \"\"\"\n",
    "\n",
    "        y_encoded = encoder.transform(y)\n",
    "        y_dummy = np_utils.to_categorical(y_encoded)\n",
    "\n",
    "        return y_dummy\n",
    "\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "\n",
    "    def define_model_keras():\n",
    "        #Define model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(500, input_dim=663, activation='relu'))\n",
    "        model.add(Dense(500, activation='relu'))\n",
    "        model.add(Dense(250, activation='relu'))\n",
    "        model.add(Dense(6, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "except:\n",
    "    print('keras part not executed')\n",
    "\n",
    "# loss='categorical_crossentropy',\n",
    "# optimizer='adam', \n",
    "# activation fns: relu x3, softmax x1\n",
    "\n",
    "\n",
    "# lab3/lab3_mlp_classif.ipynb\n",
    "# 237:    \"However, when training a classification network, we generally use the [Cross Entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) loss function, which alleviates these two issues. This loss is optimized for handling true labels instead of true probabilities per class, so you don't have to worry about it. Besides, it will automatically apply a [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) non-linearity to the predicted outputs, in order to normalize them as probabilities per class.\\n\",\n",
    "\n",
    "# https://adamoudad.github.io/posts/keras_torch_comparison/syntax/\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import copy\n",
    "\n",
    "class ClassifModel(nn.Module):\n",
    "    def __init__(self , input_size , output_size\n",
    "                 , hidden_size=500\n",
    "                 , act_fn=nn.ReLU()\n",
    "                 , n_hidden_layers=1 ):\n",
    "        super(ClassifModel, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), act_fn)\n",
    "\n",
    "        self.hiddens = nn.ModuleList([])\n",
    "        for i in range(n_hidden_layers):\n",
    "            self.hiddens.append( nn.Sequential(nn.Linear(hidden_size, hidden_size), act_fn) )\n",
    "\n",
    "        print('len of self.hiddens:', len(self.hiddens))\n",
    "\n",
    "        alt_hidden_size = 250\n",
    "        # This one is not in the hiddens loop because it has a different out_features size\n",
    "        self.hidden2 = nn.Sequential( nn.Linear(hidden_size, alt_hidden_size), act_fn )\n",
    "\n",
    "        # self.output_layer = nn.LazyLinear(output_size)\n",
    "        self.output_layer = nn.Linear(alt_hidden_size, output_size)\n",
    "\n",
    "        # NOTE: Unlike keras, we do not need a final softmax fn at the outputs,\n",
    "        # because it's not necessary with torch.nn.CrossEntropyLoss.\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#\n",
    "        # https://discuss.pytorch.org/t/do-i-need-to-use-softmax-before-nn-crossentropyloss/16739\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        if len(self.hiddens) != 0:\n",
    "            for i, l in enumerate(self.hiddens):\n",
    "                x = self.hiddens[i](x)\n",
    "\n",
    "        x = self.hidden2(x)\n",
    "        out = self.output_layer(x)\n",
    "        return out\n",
    "\n",
    "INPUT_SIZE = 663\n",
    "OUTPUT_SIZE = 6\n",
    "model = ClassifModel(INPUT_SIZE, OUTPUT_SIZE)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0979fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Define the hyperparameters to create an instance of the model \n",
    "# (e.g., hidden space size, number of convolution kernels...) \n",
    "# as well as the parameters required to train neural network (e.g., learning rate).\n",
    "#########\n",
    "\n",
    "# The Conor O'Sullivan article uses adam from Keras with default learning rate,\n",
    "# and default there is 0.001: https://keras.io/api/optimizers/adam/\n",
    "learning_rate = 0.001\n",
    "\n",
    "# lab4/lab4.2_scott.ipynb\n",
    "# 363:    \"    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\\n\",\n",
    "\n",
    "optimizer = torch.optim.Adam#(model_tr.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4183f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Write the training loop for training the model.\n",
    "#########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Evaluate the model on the test data. \n",
    "# In this part, it is expected to choose an appropriate evaluation metric based on your task. \n",
    "# For instance, for classification task, accuracy should be computed (but you can also search for 'precision' and 'recall').\n",
    "#########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732262de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Save the trained model parameters, and the obtained results if needed.\n",
    "#########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6a96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
