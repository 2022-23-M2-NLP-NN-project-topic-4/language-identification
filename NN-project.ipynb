{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd71ac1d",
   "metadata": {},
   "source": [
    "# NN Project, topic #4: Language identification from a text corpus\n",
    "\n",
    "Students: Shahzaib MUHAMMAD, Adriana NICOARA, Scott TANKARD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631d741",
   "metadata": {},
   "source": [
    "## Our group's topic\n",
    "\n",
    "From: https://arche.univ-lorraine.fr/mod/page/view.php?id=1310565\n",
    "\n",
    "Project 4: Language identification from a text corpus\n",
    "\n",
    "Students: Scott TANKARD, Adriana NICOARA, Shahzaib MUHAMMAD\n",
    "\n",
    "Task: Text classification\n",
    "\n",
    "Model: Feedforward neural network\n",
    "\n",
    "Dataset: https://www.kaggle.com/zarajamshaid/language-identification-datasst\n",
    "\n",
    "Hint: As a preprocessing step, you should transforms the sentences into trigrams at the character level (more details at: https://towardsdatascience.com/deep-neural-network-language-identification-ae1c158f6a7d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d8f63",
   "metadata": {},
   "source": [
    "## General project instructions\n",
    "\n",
    "From: https://arche.univ-lorraine.fr/mod/page/view.php?id=1301340\n",
    "\n",
    "General project instructions\n",
    "\n",
    "Project structure\n",
    "\n",
    "Each project follows the same overall structure:\n",
    "\n",
    "    Download the data and preprocess it as required for the given task (if needed).\n",
    "    Write a Dataset class for creating the train and test datasets (and corresponding dataloaders).\n",
    "    Define the neural network model.\n",
    "    Define the hyperparameters to create an instance of the model (e.g., hidden space size, number of convolution kernels...) as well as the parameters required to train neural network (e.g., learning rate).\n",
    "    Write the training loop for training the model.\n",
    "    Evaluate the model on the test data. In this part, it is expected to choose an appropriate evaluation metric based on your task. For instance, for classification task, accuracy should be computed (but you can also search for 'precision' and 'recall').\n",
    "    Save the trained model parameters, and the obtained results if needed.\n",
    "\n",
    "\n",
    "Deliverables\n",
    "\n",
    "The project should be written using Pytorch (and not Keras/Tensorflow or any other python deep learning framework). You're expected to send a zip file containing:\n",
    "\n",
    "    A python file implementing steps 1 to 7. This can be either a jupyter notebook or a .py script. A single file is preferred, but you can use several files if that's your style, as long as your main executable script is clearly indicated (for instance it's named 'main.py').\n",
    "    The obtained results as extra files (e.g., the trained model parameters, a figure with the training loss / validation metric over epochs, a graph comparing different architectures if you want to play arround with it, etc.)\n",
    "\n",
    "Your code should be commented in order to clarify implementation details, and desciption about the inputs and outputs of functions.\n",
    "\n",
    "Note: There is no need to send the dataset, since I will download it and run your scripts directly. Therefore, make sure that you do not transform/change the raw data, and if there is some preprocessing involved, include it in your python file(s).\n",
    "\n",
    "\n",
    "General Hints\n",
    "\n",
    "    The tasks corresponding to these projects have been extensively studied. Don't hesitate to search online for more information (either tutorials or research papers, even pieces of code if you can adapt it).\n",
    "    Some datasets are very large. Therefore, you don't need to use all the data, but instead you can extract a subset of it (for instance, only a few languages for language identification, only a few images/classes for image recognition etc.) to have a lighter dataset / model / training procedure.\n",
    "    Some datasets are provided with a train / test split, but not always. Either way, you can create your own split (a good rule of thumb can be 80% training and 20 % testing).\n",
    "    It is strongly advised to use validation in order to monitor training (see 'bonus work' in lab 2.2). You can use part of the training data (e.g., 10%) as a validation set.\n",
    "    It is good practice to start with a light model (very few layers/parameters) and dataset (subset of your whole dataset) for prototyping. The performance won't be very good, but it's useful to check if there are any error in the train/test procedure. Once everything runs smoothly, you can increase the size of the model and use more data.\n",
    "    If your project includes convolutional or recurrent neural networks, don't wait the corresponding lab: you can already start working on it (you can basically do everything, just using a 'dummy' MLP model instead of a CNN/RNN).\n",
    "\n",
    "Modifié le: vendredi 21 octobre 2022, 12:46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60401e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:07.343681Z",
     "start_time": "2022-11-14T23:12:04.498313Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7797bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:07.359269Z",
     "start_time": "2022-11-14T23:12:07.354192Z"
    }
   },
   "outputs": [],
   "source": [
    "#CONSTANTS:\n",
    "filterLanguage=True #Either Filter the dataset or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3fb448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:07.390255Z",
     "start_time": "2022-11-14T23:12:07.365734Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 1: Download the data and preprocess it as required for the given task (if needed).\n",
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c31d806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:08.409905Z",
     "start_time": "2022-11-14T23:12:07.394277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22000 entries, 0 to 21999\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Text      22000 non-null  object\n",
      " 1   language  22000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 343.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./dataset.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e57db9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:08.439949Z",
     "start_time": "2022-11-14T23:12:08.412980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>エノが行きがかりでバスに乗ってしまい、気分が悪くなった際に助けるが、今すぐバスを降りたいと運...</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tsutinalar i̇ngilizce tsuutina kanadada albert...</td>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>müller mox figura centralis circulorum doctoru...</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>برقی بار electric charge تمام زیرجوہری ذرات کی...</td>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>シャーリー・フィールドは、サン・ベルナルド・アベニュー沿い市民センターとrtマーティン高校に...</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  language\n",
       "0  klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
       "1  sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2  ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3  விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4  de spons behoort tot het geslacht haliclona en...     Dutch\n",
       "5  エノが行きがかりでバスに乗ってしまい、気分が悪くなった際に助けるが、今すぐバスを降りたいと運...  Japanese\n",
       "6  tsutinalar i̇ngilizce tsuutina kanadada albert...   Turkish\n",
       "7  müller mox figura centralis circulorum doctoru...     Latin\n",
       "8  برقی بار electric charge تمام زیرجوہری ذرات کی...      Urdu\n",
       "9  シャーリー・フィールドは、サン・ベルナルド・アベニュー沿い市民センターとrtマーティン高校に...  Japanese"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60848ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:08.471040Z",
     "start_time": "2022-11-14T23:12:08.448168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Estonian' 'Swedish' 'Thai' 'Tamil' 'Dutch' 'Japanese' 'Turkish' 'Latin'\n",
      " 'Urdu' 'Indonesian' 'Portugese' 'French' 'Chinese' 'Korean' 'Hindi'\n",
      " 'Spanish' 'Pushto' 'Persian' 'Romanian' 'Russian' 'English' 'Arabic']\n",
      "Number of unique languages:  22\n"
     ]
    }
   ],
   "source": [
    "unique_lang = data[\"language\"].unique()\n",
    "print(unique_lang)\n",
    "print(\"Number of unique languages: \", len(unique_lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22b2cc",
   "metadata": {},
   "source": [
    "We choose 7 languages to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84d1ff64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:08.541097Z",
     "start_time": "2022-11-14T23:12:08.474176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14693/883624736.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "/tmp/ipykernel_14693/883624736.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "/tmp/ipykernel_14693/883624736.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "/tmp/ipykernel_14693/883624736.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "/tmp/ipykernel_14693/883624736.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "/tmp/ipykernel_14693/883624736.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "/tmp/ipykernel_14693/883624736.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n"
     ]
    }
   ],
   "source": [
    "data_trim = pd.DataFrame(columns=['Text','language'])\n",
    "lang = None\n",
    "if(filterLanguage):\n",
    "    lang = ['Estonian', 'Swedish', 'Indonesian','Turkish','French', 'Romanian', 'English' ]\n",
    "    #lang = ['Estonian', 'Swedish', 'Indonesian', 'Romanian' ]\n",
    "    data_trim = pd.DataFrame(columns=['Text','language'])\n",
    "    for l in lang:\n",
    "        lang_trim = data[data['language'] ==l].sample(1000, random_state = 100)\n",
    "        data_trim = data_trim.append(lang_trim)\n",
    "else:\n",
    "    lang = data['language'].unique()\n",
    "    data_trim = data\n",
    "\n",
    "# We will need this later for output_size \n",
    "global num_languages \n",
    "num_languages = len(lang)\n",
    "print(num_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc003fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:10.002600Z",
     "start_time": "2022-11-14T23:12:08.544623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Text    language\n",
      "0     arne merilai on tegelnud mitme humanitaarteadu...    Estonian\n",
      "1     le premier catalogue de la nouvelle galerie es...      French\n",
      "2     i omgivningarna runt mambi växer i huvudsak st...     Swedish\n",
      "3      mayıs te reloaded ubisoftun çok beklenen açık...     Turkish\n",
      "4     ketika anak kucing sedang menyusui mereka past...  Indonesian\n",
      "...                                                 ...         ...\n",
      "5945  küberkiusamine leiab aset kui lapsed või noore...    Estonian\n",
      "5946  grupul francez laboratoires urgo are două divi...    Romanian\n",
      "5947  in  boom studios began publishing an escape fr...     English\n",
      "5948  la pratique sest ensuite largement diffusée no...      French\n",
      "5949  istilah paspor sendiri berasal berasal dari se...  Indonesian\n",
      "\n",
      "[5950 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# STEP 1B: Write a Dataset class for creating the train and test datasets.\n",
    "#########\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data_trim[\"Text\"]\n",
    "y = data_trim[\"language\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42,stratify=y)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19b3b7be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:10.066021Z",
     "start_time": "2022-11-14T23:12:10.006582Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_trigrams(corpus,n_feat=200):\n",
    "    \"\"\"\n",
    "    Returns a list of the N most common character trigrams from a list of sentences\n",
    "    params\n",
    "    ------------\n",
    "        corpus: list of strings\n",
    "        n_feat: integer\n",
    "    \"\"\"\n",
    "    #fit the n-gram model\n",
    "    vectorizer = CountVectorizer(analyzer='char_wb',\n",
    "                            ngram_range=(3, 3),\n",
    "                            max_features=n_feat)\n",
    "    \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    #Get model feature names\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37241ff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:11.813518Z",
     "start_time": "2022-11-14T23:12:10.074594Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/NN_proj/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "features_set = set()\n",
    "\n",
    "for l in lang:\n",
    "    #get corpus filtered by language\n",
    "    corpus = train_data[train_data.language==l]['Text']\n",
    "    #get 200 most frequent trigrams\n",
    "    trigrams = get_trigrams(corpus)\n",
    "    \n",
    "    #add to dict and set\n",
    "    features[l] = trigrams \n",
    "    features_set.update(trigrams)\n",
    "#create vocabulary list using feature set\n",
    "vocab = dict()\n",
    "for i,f in enumerate(features_set):\n",
    "    vocab[f]=i\n",
    "    \n",
    "\n",
    "# We will need this later for input_size \n",
    "global num_trigram_features \n",
    "num_trigram_features = len(vocab)\n",
    "print(num_trigram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78d0af69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:13.434096Z",
     "start_time": "2022-11-14T23:12:11.820211Z"
    }
   },
   "outputs": [],
   "source": [
    "#train count vectoriser using vocabulary\n",
    "vectorizer = CountVectorizer(analyzer='char_wb',\n",
    "                             ngram_range=(3, 3),\n",
    "                             vocabulary=vocab)\n",
    "\n",
    "#create feature matrix for training set\n",
    "corpus = train_data['Text']   \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "train_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "533c7831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:13.469952Z",
     "start_time": "2022-11-14T23:12:13.436601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apa</th>\n",
       "      <th>lii</th>\n",
       "      <th>işt</th>\n",
       "      <th>uds</th>\n",
       "      <th>ınd</th>\n",
       "      <th>oms</th>\n",
       "      <th>rna</th>\n",
       "      <th>emi</th>\n",
       "      <th>i̇</th>\n",
       "      <th>ava</th>\n",
       "      <th>...</th>\n",
       "      <th>a</th>\n",
       "      <th>til</th>\n",
       "      <th>ec</th>\n",
       "      <th>oma</th>\n",
       "      <th>aa</th>\n",
       "      <th>cal</th>\n",
       "      <th>aas</th>\n",
       "      <th>att</th>\n",
       "      <th>pr</th>\n",
       "      <th>ly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5945</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5946</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5949</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5950 rows × 822 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      apa  lii  işt  uds  ınd  oms  rna  emi   i̇  ava  ...   a   til  ec   \\\n",
       "0       0    1    0    0    0    0    0    0    0    0  ...    0    1    0   \n",
       "1       0    0    0    0    0    0    0    1    0    0  ...    0    0    0   \n",
       "2       0    0    0    1    0    2    1    0    0    1  ...    0    0    0   \n",
       "3       0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "4       0    0    0    0    0    0    0    4    0    0  ...    0    0    0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "5945    0    0    0    0    0    0    0    0    0    8  ...    0    0    0   \n",
       "5946    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "5947    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "5948    0    0    0    0    0    0    0    1    0    0  ...    0    0    0   \n",
       "5949    0    0    0    0    0    0    0    1    0    0  ...    0    1    0   \n",
       "\n",
       "      oma   aa  cal  aas  att   pr  ly   \n",
       "0       1    0    0    0    0    1    0  \n",
       "1       0    0    0    0    0    1    0  \n",
       "2       0    0    0    0    0    0    0  \n",
       "3       0    0    0    0    0    0    0  \n",
       "4       0    0    0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "5945    0    0    0    0    0    0    0  \n",
       "5946    0    0    1    0    0    3    0  \n",
       "5947    0    0    0    0    0    0    0  \n",
       "5948    0    0    0    0    0    2    0  \n",
       "5949    0    0    0    0    0    0    0  \n",
       "\n",
       "[5950 rows x 822 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c6388f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:13.643614Z",
     "start_time": "2022-11-14T23:12:13.472420Z"
    }
   },
   "outputs": [],
   "source": [
    "#Scale feature matrix \n",
    "train_min = train_feat.min()\n",
    "train_max = train_feat.max()\n",
    "train_feat = (train_feat - train_min)/(train_max-train_min)\n",
    "\n",
    "#Add target variable \n",
    "#train_feat['lang'] = list(train_data['language'])\n",
    "\n",
    "\n",
    "# ADDDED BY SCOTT\n",
    "#Add target variable as numeric value (not string) \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def fit_encoder(lang_labels):\n",
    "    #Fit encoder\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(lang_labels)\n",
    "    return encoder\n",
    "\n",
    "def encode(y, encoder):\n",
    "    \"\"\"\n",
    "    Returns a list of one hot encodings\n",
    "    Params\n",
    "    ---------\n",
    "        y: list of language labels\n",
    "    \"\"\"\n",
    "\n",
    "    y_encoded = encoder.transform(y)\n",
    "    y_dummy = np_utils.to_categorical(y_encoded)\n",
    "\n",
    "    return y_dummy\n",
    "\n",
    "\n",
    "#encoder = fit_encoder(list(train_data['language']))\n",
    "#encoded = encode(train_data['language'], encoder)\n",
    "#train_feat['lang'] = list( encoded )\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "#le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "#list(le.classes_)\n",
    "#le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
    "#list(le.inverse_transform([2, 2, 1]))\n",
    "\n",
    "le.fit(list(train_data['language']))\n",
    "encoded = le.transform( list(train_data['language']) )\n",
    "train_feat['lang'] = list( encoded )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549261c",
   "metadata": {},
   "source": [
    "Matrix data that will be used for training: train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4683ccf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:13.818745Z",
     "start_time": "2022-11-14T23:12:13.657629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apa</th>\n",
       "      <th>lii</th>\n",
       "      <th>işt</th>\n",
       "      <th>uds</th>\n",
       "      <th>ınd</th>\n",
       "      <th>oms</th>\n",
       "      <th>rna</th>\n",
       "      <th>emi</th>\n",
       "      <th>i̇</th>\n",
       "      <th>ava</th>\n",
       "      <th>...</th>\n",
       "      <th>a</th>\n",
       "      <th>til</th>\n",
       "      <th>ec</th>\n",
       "      <th>oma</th>\n",
       "      <th>aa</th>\n",
       "      <th>cal</th>\n",
       "      <th>aas</th>\n",
       "      <th>att</th>\n",
       "      <th>pr</th>\n",
       "      <th>ly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5945</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5946</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5949</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5950 rows × 822 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      apa       lii  işt  uds  ınd       oms       rna    emi   i̇    ava  \\\n",
       "0     0.0  0.166667  0.0  0.0  0.0  0.000000  0.000000  0.000  0.0  0.000   \n",
       "1     0.0  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.125  0.0  0.000   \n",
       "2     0.0  0.000000  0.0  0.5  0.0  0.666667  0.111111  0.000  0.0  0.125   \n",
       "3     0.0  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.000  0.0  0.000   \n",
       "4     0.0  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.500  0.0  0.000   \n",
       "...   ...       ...  ...  ...  ...       ...       ...    ...  ...    ...   \n",
       "5945  0.0  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.000  0.0  1.000   \n",
       "5946  0.0  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.000  0.0  0.000   \n",
       "5947  0.0  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.000  0.0  0.000   \n",
       "5948  0.0  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.125  0.0  0.000   \n",
       "5949  0.0  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.125  0.0  0.000   \n",
       "\n",
       "      ...   a   til  ec        oma   aa    cal  aas  att        pr  ly   \n",
       "0     ...  0.0  0.2  0.0  0.166667  0.0  0.000  0.0  0.0  0.083333  0.0  \n",
       "1     ...  0.0  0.0  0.0  0.000000  0.0  0.000  0.0  0.0  0.083333  0.0  \n",
       "2     ...  0.0  0.0  0.0  0.000000  0.0  0.000  0.0  0.0  0.000000  0.0  \n",
       "3     ...  0.0  0.0  0.0  0.000000  0.0  0.000  0.0  0.0  0.000000  0.0  \n",
       "4     ...  0.0  0.0  0.0  0.000000  0.0  0.000  0.0  0.0  0.000000  0.0  \n",
       "...   ...  ...  ...  ...       ...  ...    ...  ...  ...       ...  ...  \n",
       "5945  ...  0.0  0.0  0.0  0.000000  0.0  0.000  0.0  0.0  0.000000  0.0  \n",
       "5946  ...  0.0  0.0  0.0  0.000000  0.0  0.125  0.0  0.0  0.250000  0.0  \n",
       "5947  ...  0.0  0.0  0.0  0.000000  0.0  0.000  0.0  0.0  0.000000  0.0  \n",
       "5948  ...  0.0  0.0  0.0  0.000000  0.0  0.000  0.0  0.0  0.166667  0.0  \n",
       "5949  ...  0.0  0.2  0.0  0.000000  0.0  0.000  0.0  0.0  0.000000  0.0  \n",
       "\n",
       "[5950 rows x 822 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat\n",
    "\n",
    "\n",
    "blah = train_feat.drop('lang', axis=1)\n",
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65769cda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:14.030674Z",
     "start_time": "2022-11-14T23:12:13.828191Z"
    }
   },
   "outputs": [],
   "source": [
    "#Train and validation split\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42,stratify=y_train)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_feat.drop('lang', axis=1), \n",
    "                                                  train_feat['lang'], \n",
    "                                                  test_size=0.10, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=train_feat['lang'])\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_val.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "\n",
    "#SCOTT ADDED\n",
    "#train_data = [X_train, y_train]\n",
    "#val_data = pd.concat([X_val, y_val], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ab01f",
   "metadata": {},
   "source": [
    "Heatmap representing the number of common trigrams between languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76ad5861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:15.649219Z",
     "start_time": "2022-11-14T23:12:14.038476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Calculate number of shared trigrams\n",
    "labels = lang\n",
    "mat = []\n",
    "for i in labels:\n",
    "    vec = []\n",
    "    for j in labels:\n",
    "        l1 = features[i]\n",
    "        l2 = features[j]\n",
    "        intersec = [l for l in l1 if l in l2] \n",
    "        # print(intersec)\n",
    "        vec.append(len(intersec))\n",
    "    mat.append(vec)\n",
    "\n",
    "#Plot heatmap\n",
    "conf_matrix_df = pd.DataFrame(mat,columns=lang,index=lang)\n",
    "# Create a mask\n",
    "mask = np.triu(np.ones_like(conf_matrix_df, dtype=bool))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10), facecolor='w', edgecolor='k')\n",
    "# sns.set(font_scale=1.5)\n",
    "sns.heatmap(conf_matrix_df,cmap='coolwarm',annot=True,fmt='.5g',cbar=False, mask=mask,)\n",
    "\n",
    "plt.savefig('./feat_explore.png',format='png',dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bca6bf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:15.660744Z",
     "start_time": "2022-11-14T23:12:15.652071Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 2: Write a Dataset class for creating the train and test datasets (and corresponding dataloaders).\n",
    "#########\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9c71077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:16.280036Z",
     "start_time": "2022-11-14T23:12:15.663696Z"
    }
   },
   "outputs": [],
   "source": [
    "# SCOTT ADDED this cell\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# https://discuss.pytorch.org/t/typeerror-batch-must-contain-tensors-numbers-dicts-or-lists-found-object/14665/3\n",
    "#For example add to the __init__:\n",
    "# self.transform = transforms.Compose([transforms.ToTensor()])  \n",
    "#and in __getitem__ do:\n",
    "# return self.transform(self.x_data[index]), self.transform(self.y_data[index])\n",
    "\n",
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        stuff = self.dataframe.iloc[index]\n",
    "        return self.transform(stuff)\n",
    "    \n",
    "\n",
    "# https://stackoverflow.com/questions/58612453/keyerror-when-enumerating-over-dataloader\n",
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataframe.iloc[index]\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/58612453/keyerror-when-enumerating-over-dataloader\n",
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        thing = self.dataframe.iloc[index]\n",
    "        thing = thing.tolist()\n",
    "        print(thing)\n",
    "        return thing\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/58612453/keyerror-when-enumerating-over-dataloader\n",
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        thing = self.dataframe.iloc[index]\n",
    "        \n",
    "        t1 = thing.drop('lang').astype(np.float32)\n",
    "        t1 = t1.tolist()\n",
    "        t1 = torch.tensor(t1)\n",
    "        \n",
    "        t2 = thing['lang'].astype(np.int64)\n",
    "        \n",
    "        thing = [t1,t2]\n",
    "        #print(thing)\n",
    "        return thing\n",
    "\n",
    "    \n",
    "# SCOTT ADDED\n",
    "train_dataloader = DataLoader(PandasDataset(train_data), shuffle=True, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(PandasDataset(val_data), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "694e28e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:16.640249Z",
     "start_time": "2022-11-14T23:12:16.282564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras part not executed\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Define the neural network model.\n",
    "#########\n",
    "\n",
    "# https://towardsdatascience.com/deep-neural-network-language-identification-ae1c158f6a7d\n",
    "\n",
    "try:\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from keras.utils import np_utils\n",
    "\n",
    "    def fit_encoder():\n",
    "        #Fit encoder\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(['deu', 'eng', 'fra', 'ita', 'por', 'spa'])\n",
    "\n",
    "    def encode(y):\n",
    "        \"\"\"\n",
    "        Returns a list of one hot encodings\n",
    "        Params\n",
    "        ---------\n",
    "            y: list of language labels\n",
    "        \"\"\"\n",
    "\n",
    "        y_encoded = encoder.transform(y)\n",
    "        y_dummy = np_utils.to_categorical(y_encoded)\n",
    "\n",
    "        return y_dummy\n",
    "\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "\n",
    "    def define_model_keras():\n",
    "        #Define model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(500, input_dim=663, activation='relu'))\n",
    "        model.add(Dense(500, activation='relu'))\n",
    "        model.add(Dense(250, activation='relu'))\n",
    "        model.add(Dense(6, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "except:\n",
    "    print('keras part not executed')\n",
    "\n",
    "# loss='categorical_crossentropy',\n",
    "# optimizer='adam', \n",
    "# activation fns: relu x3, softmax x1\n",
    "\n",
    "\n",
    "# lab3/lab3_mlp_classif.ipynb\n",
    "# 237:    \"However, when training a classification network, we generally use the [Cross Entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) loss function, which alleviates these two issues. This loss is optimized for handling true labels instead of true probabilities per class, so you don't have to worry about it. Besides, it will automatically apply a [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) non-linearity to the predicted outputs, in order to normalize them as probabilities per class.\\n\",\n",
    "\n",
    "# https://adamoudad.github.io/posts/keras_torch_comparison/syntax/\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import copy\n",
    "\n",
    "class ClassifModel(nn.Module):\n",
    "    def __init__(self , input_size , output_size\n",
    "                 , hidden_size=500\n",
    "                 , act_fn=nn.ReLU()\n",
    "                 , n_hidden_layers=1 ):\n",
    "        super(ClassifModel, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), act_fn)\n",
    "\n",
    "        self.hiddens = nn.ModuleList([])\n",
    "        for i in range(n_hidden_layers):\n",
    "            self.hiddens.append( nn.Sequential(nn.Linear(hidden_size, hidden_size), act_fn) )\n",
    "\n",
    "        print('len of self.hiddens:', len(self.hiddens))\n",
    "\n",
    "        alt_hidden_size = 250\n",
    "        # This one is not in the hiddens loop because it has a different out_features size\n",
    "        self.hidden2 = nn.Sequential( nn.Linear(hidden_size, alt_hidden_size), act_fn )\n",
    "\n",
    "        # self.output_layer = nn.LazyLinear(output_size)\n",
    "        self.output_layer = nn.Linear(alt_hidden_size, output_size)\n",
    "\n",
    "        # NOTE: Unlike keras, we do not need a final softmax fn at the outputs,\n",
    "        # because it's not necessary with torch.nn.CrossEntropyLoss.\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#\n",
    "        # https://discuss.pytorch.org/t/do-i-need-to-use-softmax-before-nn-crossentropyloss/16739\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        if len(self.hiddens) != 0:\n",
    "            for i, l in enumerate(self.hiddens):\n",
    "                x = self.hiddens[i](x)\n",
    "\n",
    "        x = self.hidden2(x)\n",
    "        out = self.output_layer(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "def test_model():\n",
    "    INPUT_SIZE = 663\n",
    "    output_size = num_languages # e.g. 7 or 22\n",
    "    model = ClassifModel(INPUT_SIZE, output_size)\n",
    "    return model\n",
    "\n",
    "#test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d0979fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:16.646819Z",
     "start_time": "2022-11-14T23:12:16.643018Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 4: Define the hyperparameters to create an instance of the model \n",
    "# (e.g., hidden space size, number of convolution kernels...) \n",
    "# as well as the parameters required to train neural network (e.g., learning rate).\n",
    "#########\n",
    "\n",
    "# The Conor O'Sullivan article uses adam from Keras with default learning rate,\n",
    "# and default there is 0.001: https://keras.io/api/optimizers/adam/\n",
    "learning_rate = 0.001\n",
    "\n",
    "# lab4/lab4.2_scott.ipynb\n",
    "# 363:    \"    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\\n\",\n",
    "\n",
    "optimizer = torch.optim.Adam#(model_tr.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e081293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:16.667710Z",
     "start_time": "2022-11-14T23:12:16.650409Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 5: Write the training loop for training the model.\n",
    "#########\n",
    "\n",
    "\n",
    "##########################\n",
    "#from lab 4.1\n",
    "\n",
    "# Evaluation function: similar to the training loop, except we don't need to compute any gradient / backprop\n",
    "def eval_mlp_classifier(model, eval_dataloader):\n",
    "    \n",
    "    # Set the model in 'evaluation' mode (this disables some layers (batch norm, dropout...) which are not needed when testing)\n",
    "    model.eval() \n",
    "\n",
    "    # In evaluation phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "        \n",
    "        # Iterate over the dataset using the dataloader\n",
    "        for images, labels in eval_dataloader:\n",
    "\n",
    "            # Get the predicted labels\n",
    "            \n",
    "            #images = images.reshape(images.shape[0], -1)\n",
    "            y_predicted = model(images)\n",
    "            \n",
    "            # To get the predicted labels, we need to get the max over all possible classes\n",
    "            _, label_predicted = torch.max(y_predicted.data, 1)\n",
    "            \n",
    "            # Compute accuracy: count the total number of samples, and the correct labels (compare the true and predicted labels)\n",
    "            total_labels += labels.size(0)\n",
    "            correct_labels += (label_predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct_labels / total_labels\n",
    "    \n",
    "    return accuracy\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "#from lab 4.1\n",
    "\n",
    "# TO DO: write the training function with validation\n",
    "# - the overall training procedure is similar to what was done in lab 3 (reuse it)\n",
    "# - at the end of each epoch, compute the accuracy of the model on the validation subset using the provided evaluation function\n",
    "# - then, check if this accuracy is increasing: if so, then we save the current model as the 'model_opt'.\n",
    "# - return as output the trained model, the training loss, and the validation accuracy\n",
    "##############################\n",
    "\n",
    "def pltListOverEpochs(losses, title='Loss over epochs', ylab='loss', xlab='epoch'):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    epochs = range(1, len(losses)+1)\n",
    "    plt.plot(epochs, losses, 'g+')\n",
    "    plt.xlabel(xlab), plt.ylabel(ylab)\n",
    "    plt.show()\n",
    "\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4183f60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:12:16.695921Z",
     "start_time": "2022-11-14T23:12:16.672910Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "# from lab 4.2\n",
    "def train_val_classifier(model, train_dataloader, eval_dataloader, \n",
    "                         num_epochs, loss_fn, learning_rate, verbose=True):\n",
    "    \"\"\"Training function with validation. \n",
    "    \n",
    "    (Similar to lab 4.2 etc). \n",
    "    Important NOTE: if we compute the validation loss, it should decrease. If we compute accuracy, it should increase.\n",
    "\"\"\"\n",
    "\n",
    "    # Make a copy of the model (avoid changing the model outside this function)\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    model = None # for safety\n",
    "    model_tr.train() # Set the model in 'training' mode \n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize a list to record the training loss over epochs\n",
    "    loss_all_epochs = []\n",
    "    altLossAllEpochs = [] # Alternate method, loss\n",
    "    evalAllEpochs = [] # list of validation metric (e.g. loss or accuracy)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the training loss for the current epoch\n",
    "        loss_current_epoch = 0\n",
    "        altLCEs = [] # Alternate method\n",
    "        \n",
    "        #print(train_dataloader)\n",
    "        \n",
    "        # Iterate over batches using the dataloader\n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "            # training procedure for each batch...\n",
    "            #print(batch_index, images, labels)\n",
    "            \n",
    "            # - vectorizing the images (size should be (batch_size, input_size))\n",
    "            #batch_size = 8\n",
    "            #input_size = num_languages\n",
    "            #vectorized_batch = images.reshape(batch_size, input_size)\n",
    "            #print(vectorized_batch.shape)\n",
    "            \n",
    "            \n",
    "            # we are not using images this time # TBD clean this up\n",
    "            #vectorized_batch = torch.tensor(np.array( images[0] )) \n",
    "            \n",
    "            #vectorized_batch = torch.tensor( np.fromstring( (images[0]) , dtype=int ) )\n",
    "            \n",
    "            #tensor_ = torch.from_numpy( images.to_numpy().astype(np.float32))\n",
    "            #vectorized_batch = tensor_\n",
    "            \n",
    "            #images = images.reshape(images.shape[0], -1)\n",
    "            \n",
    "            vectorized_batch = images\n",
    "            \n",
    "            \n",
    "            y = labels # the prediction targets \n",
    "            y_pred = model_tr(vectorized_batch) #step1, forward pass (calculate predicted labels)\n",
    "            \n",
    "            #print(y_pred, y)\n",
    "            \n",
    "            l = loss_fn(y_pred, y) #step2\n",
    "            optimizer.zero_grad()\n",
    "            l.backward() # step3, compute the gradients\n",
    "            optimizer.step() #step4, (?apply gradient descent algorithm?)\n",
    "            \n",
    "            # update the loss at the current epoch\n",
    "            loss_current_epoch += l.item()\n",
    "            #print(batch_index, '::', l.item() )\n",
    "            \n",
    "            altLCEs.append(l.item())\n",
    "            nBatchesThisEpoch = batch_index + 1\n",
    "\n",
    "        # At the end of each epoch, record and display the loss over all batches\n",
    "        loss_all_epochs.append(loss_current_epoch)\n",
    "        \n",
    "        altLossAvgThisEpoch = sum(altLCEs) / nBatchesThisEpoch\n",
    "        altLossAllEpochs.append(altLossAvgThisEpoch)\n",
    "        \n",
    "        # at end of each epoch, compute the validation metric (e.g. accuracy, loss) of the model on validation subset \n",
    "        this_eval = eval_mlp_classifier(model_tr, eval_dataloader)\n",
    "        evalAllEpochs.append(this_eval)\n",
    "        #last_eval = evalAllEpochs[epoch-1]\n",
    "\n",
    "        #print( 'this eval {}, min eval {}'.format(this_eval, min(evalAllEpochs)) )\n",
    "        # is eval metric improving? if so, then we save the current model as the 'model_opt'.\n",
    "        if len(evalAllEpochs) == 1:\n",
    "            print('only one eval done yet')\n",
    "            model_opt = model_tr\n",
    "        elif (this_eval >= min(evalAllEpochs)) and (this_eval not in evalAllEpochs[:-1]):\n",
    "            # <= for \"is LOSS DEcreasing\", or >= for \"is accuracy increasing\"\n",
    "            print('saving model from epoch {} as new model_opt'.format(epoch+1))\n",
    "            model_opt = model_tr\n",
    "\n",
    "        if verbose:\n",
    "            #print('TRAINING Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch))\n",
    "            print('TRAINING Epoch [{}/{}], Loss: {:.4f}, altLoss: {:.4f}, this_eval: {:.4f}'.format(\n",
    "                epoch+1, num_epochs, loss_current_epoch, altLossAvgThisEpoch, this_eval\n",
    "            ))\n",
    "            #print(nBatchesThisEpoch)\n",
    "\n",
    "    training_loss = altLossAllEpochs\n",
    "    validation_metric = evalAllEpochs\n",
    "    \n",
    "    return model_opt, training_loss, validation_metric\n",
    "    #return model_tr, altLossAllEpochs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a282322e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:15:26.949762Z",
     "start_time": "2022-11-14T23:12:16.699390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of self.hiddens: 1\n",
      "only one eval done yet\n",
      "TRAINING Epoch [1/10], Loss: 90.0790, altLoss: 0.1344, this_eval: 98.1513\n",
      "TRAINING Epoch [2/10], Loss: 17.4865, altLoss: 0.0261, this_eval: 98.1513\n",
      "saving model from epoch 3 as new model_opt\n",
      "TRAINING Epoch [3/10], Loss: 7.0016, altLoss: 0.0105, this_eval: 97.9832\n",
      "saving model from epoch 4 as new model_opt\n",
      "TRAINING Epoch [4/10], Loss: 9.7142, altLoss: 0.0145, this_eval: 98.6555\n",
      "TRAINING Epoch [5/10], Loss: 1.1110, altLoss: 0.0017, this_eval: 98.1513\n",
      "TRAINING Epoch [6/10], Loss: 5.4610, altLoss: 0.0082, this_eval: 97.9832\n",
      "saving model from epoch 7 as new model_opt\n",
      "TRAINING Epoch [7/10], Loss: 2.5572, altLoss: 0.0038, this_eval: 98.8235\n",
      "TRAINING Epoch [8/10], Loss: 0.0654, altLoss: 0.0001, this_eval: 98.8235\n",
      "TRAINING Epoch [9/10], Loss: 0.0043, altLoss: 0.0000, this_eval: 98.8235\n",
      "TRAINING Epoch [10/10], Loss: 0.0017, altLoss: 0.0000, this_eval: 98.8235\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Evaluate the model on the test data. \n",
    "# In this part, it is expected to choose an appropriate evaluation metric based on your task. \n",
    "# For instance, for classification task, accuracy should be computed (but you can also search for 'precision' and 'recall').\n",
    "#########\n",
    "\n",
    "# Execute the training (with validation)\n",
    "\n",
    "LOSS_FN = torch.nn.CrossEntropyLoss\n",
    "OPTIMIZER = torch.optim.Adam \n",
    "# invoked as so: torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "# currently this is hardcoded inside train_val_classifier()\n",
    "\n",
    "if True:\n",
    "    batch_size = 8\n",
    "    train_dataloader = train_dataloader\n",
    "    val_dataloader = val_dataloader\n",
    "\n",
    "    input_size = num_trigram_features\n",
    "    output_size = num_languages # e.g. 7 or 22\n",
    "    \n",
    "    model = ClassifModel(input_size, output_size)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    loss_fn = LOSS_FN \n",
    "    learning_rate = learning_rate\n",
    "    \n",
    "    optimizer = OPTIMIZER\n",
    "    \n",
    "    mod, losses, evals = train_val_classifier(model, train_dataloader, val_dataloader, \n",
    "                                              num_epochs, loss_fn(), learning_rate, \n",
    "                                              verbose=True )\n",
    "\n",
    "\n",
    "    \n",
    "# Finally, execute testing/eval also on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65e691e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:19:34.202685Z",
     "start_time": "2022-11-14T23:19:34.196995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifModel(\n",
      "  (input_layer): Sequential(\n",
      "    (0): Linear(in_features=822, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hiddens): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=500, out_features=500, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (hidden2): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (output_layer): Linear(in_features=250, out_features=7, bias=True)\n",
      ")\n",
      "[0.1344463285128678, 0.026099318672639652, 0.010450133734858138, 0.014498848631949902, 0.00165828186378907, 0.00815070304471615, 0.003816772701643994, 9.758134179967545e-05, 6.452492934452701e-06, 2.476141121743325e-06]\n",
      "[98.15126050420169, 98.15126050420169, 97.98319327731092, 98.65546218487395, 98.15126050420169, 97.98319327731092, 98.82352941176471, 98.82352941176471, 98.82352941176471, 98.82352941176471]\n"
     ]
    }
   ],
   "source": [
    "print(mod)\n",
    "print(losses)\n",
    "print(evals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "732262de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T23:15:26.958977Z",
     "start_time": "2022-11-14T23:15:26.953477Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 7: Save the trained model parameters, and the obtained results if needed.\n",
    "#########\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ae826e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "len of self.hiddens: 1\n",
    "only one eval done yet\n",
    "TRAINING Epoch [1/10], Loss: 96.4513, altLoss: 0.1440, this_eval: 98.4874\n",
    "saving model from epoch 2 as new model_opt\n",
    "TRAINING Epoch [2/10], Loss: 17.3390, altLoss: 0.0259, this_eval: 97.3109\n",
    "saving model from epoch 3 as new model_opt\n",
    "TRAINING Epoch [3/10], Loss: 8.8517, altLoss: 0.0132, this_eval: 98.3193\n",
    "saving model from epoch 4 as new model_opt\n",
    "TRAINING Epoch [4/10], Loss: 3.8096, altLoss: 0.0057, this_eval: 98.1513\n",
    "TRAINING Epoch [5/10], Loss: 0.5370, altLoss: 0.0008, this_eval: 98.1513\n",
    "TRAINING Epoch [6/10], Loss: 0.0086, altLoss: 0.0000, this_eval: 98.1513\n",
    "TRAINING Epoch [7/10], Loss: 0.0037, altLoss: 0.0000, this_eval: 98.1513\n",
    "TRAINING Epoch [8/10], Loss: 0.0023, altLoss: 0.0000, this_eval: 98.3193\n",
    "TRAINING Epoch [9/10], Loss: 0.0013, altLoss: 0.0000, this_eval: 98.1513\n",
    "saving model from epoch 10 as new model_opt\n",
    "TRAINING Epoch [10/10], Loss: 0.0008, altLoss: 0.0000, this_eval: 97.9832\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "len of self.hiddens: 1\n",
    "only one eval done yet\n",
    "TRAINING Epoch [1/10], Loss: 462.8808, altLoss: 0.2200, this_eval: 97.9679\n",
    "saving model from epoch 2 as new model_opt\n",
    "TRAINING Epoch [2/10], Loss: 121.9518, altLoss: 0.0580, this_eval: 98.3422\n",
    "saving model from epoch 3 as new model_opt\n",
    "TRAINING Epoch [3/10], Loss: 69.6424, altLoss: 0.0331, this_eval: 98.5561\n",
    "saving model from epoch 4 as new model_opt\n",
    "TRAINING Epoch [4/10], Loss: 54.3106, altLoss: 0.0258, this_eval: 98.6096\n",
    "TRAINING Epoch [5/10], Loss: 27.7823, altLoss: 0.0132, this_eval: 98.3422\n",
    "saving model from epoch 6 as new model_opt\n",
    "TRAINING Epoch [6/10], Loss: 34.5034, altLoss: 0.0164, this_eval: 98.2888\n",
    "saving model from epoch 7 as new model_opt\n",
    "TRAINING Epoch [7/10], Loss: 37.0910, altLoss: 0.0176, this_eval: 98.5027\n",
    "saving model from epoch 8 as new model_opt\n",
    "TRAINING Epoch [8/10], Loss: 19.7543, altLoss: 0.0094, this_eval: 98.4492\n",
    "saving model from epoch 9 as new model_opt\n",
    "TRAINING Epoch [9/10], Loss: 19.1647, altLoss: 0.0091, this_eval: 98.0214\n",
    "TRAINING Epoch [10/10], Loss: 14.6791, altLoss: 0.0070, this_eval: 98.2888\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Useful references\n",
    "\n",
    "https://towardsdatascience.com/how-to-set-up-anaconda-and-jupyter-notebook-the-right-way-de3b7623ea4a  \n",
    "\n",
    "\n",
    "### \n",
    "\n",
    "https://stackoverflow.com/questions/20186344/importing-an-ipynb-file-from-another-ipynb-file  \n",
    "\n",
    "https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Importing%20Notebooks.html  \n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "https://stackoverflow.com/questions/50307707/how-do-i-convert-a-pandas-dataframe-to-a-pytorch-tensor\n",
    "\n",
    "https://stackoverflow.com/questions/55724123/typeerror-cant-convert-np-ndarray-of-type-numpy-object\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/47488598/attributeerror-crossentropyloss-object-has-no-attribute-backward\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "\n",
    "https://lightrun.com/answers/biocore-ntnu-pyranges-deprecation-of-nplong-in-numpy-120-in-merge-method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5093b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NN_proj]",
   "language": "python",
   "name": "conda-env-NN_proj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
